
https://github.com/BVLC/caffe/pull/4464


diff --git a/include/caffe/util/math_functions.hpp b/include/caffe/util/math_functions.hpp
index 6f6d3fe..5e07239 100644
--- a/include/caffe/util/math_functions.hpp
+++ b/include/caffe/util/math_functions.hpp
@@ -56,15 +56,24 @@ template <typename Dtype>
 void caffe_add(const int N, const Dtype* a, const Dtype* b, Dtype* y);
 
 template <typename Dtype>
+void caffe_add_to(const int N, Dtype* a, const Dtype* b);
+
+template <typename Dtype>
 void caffe_sub(const int N, const Dtype* a, const Dtype* b, Dtype* y);
 
 template <typename Dtype>
 void caffe_mul(const int N, const Dtype* a, const Dtype* b, Dtype* y);
 
 template <typename Dtype>
+void caffe_mul_to(const int N, Dtype* a, const Dtype* b);
+
+template <typename Dtype>
 void caffe_div(const int N, const Dtype* a, const Dtype* b, Dtype* y);
 
 template <typename Dtype>
+void caffe_div_to(const int N, Dtype* a, const Dtype* b);
+
+template <typename Dtype>
 void caffe_powx(const int n, const Dtype* a, const Dtype b, Dtype* y);
 
 unsigned int caffe_rng_rand();
diff --git a/include/caffe/util/mkl_alternate.hpp b/include/caffe/util/mkl_alternate.hpp
index 3355b66..4c44a76 100644
--- a/include/caffe/util/mkl_alternate.hpp
+++ b/include/caffe/util/mkl_alternate.hpp
@@ -77,6 +77,28 @@ DEFINE_VSL_BINARY_FUNC(Sub, y[i] = a[i] - b[i]);
 DEFINE_VSL_BINARY_FUNC(Mul, y[i] = a[i] * b[i]);
 DEFINE_VSL_BINARY_FUNC(Div, y[i] = a[i] / b[i]);
 
+
+// A simple way to define the vsl binary functions. The operation should
+// be in the form e.g. a[i] = a[i] + b[i]
+#define DEFINE_VSL_BINARY_TO_FUNC(name, operation) \
+  template<typename Dtype> \
+  void v##name(const int n, Dtype* a, const Dtype* b) { \
+    CHECK_GT(n, 0); CHECK(a); CHECK(b); \
+    for (int i = 0; i < n; ++i) { operation; } \
+  } \
+  inline void vs##name( \
+    const int n, float* a, const float* b) { \
+    v##name<float>(n, a, b); \
+  } \
+  inline void vd##name( \
+      const int n, double* a, const double* b) { \
+    v##name<double>(n, a, b); \
+  }
+
+DEFINE_VSL_BINARY_TO_FUNC(Add_to, a[i] = a[i] + b[i]);
+DEFINE_VSL_BINARY_TO_FUNC(Mul_to, a[i] = a[i] * b[i]);
+DEFINE_VSL_BINARY_TO_FUNC(Div_to, a[i] = a[i] / b[i]);
+
 // In addition, MKL comes with an additional function axpby that is not present
 // in standard blas. We will simply use a two-step (inefficient, of course) way
 // to mimic that.
diff --git a/src/caffe/layers/absval_layer.cpp b/src/caffe/layers/absval_layer.cpp
index 855bf0b..572f484 100644
--- a/src/caffe/layers/absval_layer.cpp
+++ b/src/caffe/layers/absval_layer.cpp
@@ -30,7 +30,7 @@ void AbsValLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
     const Dtype* bottom_data = bottom[0]->cpu_data();
     Dtype* bottom_diff = bottom[0]->mutable_cpu_diff();
     caffe_cpu_sign(count, bottom_data, bottom_diff);
-    caffe_mul(count, bottom_diff, top_diff, bottom_diff);
+    caffe_mul_to(count, bottom_diff, top_diff);
   }
 }
 
diff --git a/src/caffe/layers/batch_norm_layer.cpp b/src/caffe/layers/batch_norm_layer.cpp
index a69d8f9..c725eb7 100644
--- a/src/caffe/layers/batch_norm_layer.cpp
+++ b/src/caffe/layers/batch_norm_layer.cpp
@@ -146,7 +146,7 @@ void BatchNormLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
   caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, channels_ * num,
       spatial_dim, 1, 1., num_by_chans_.cpu_data(),
       spatial_sum_multiplier_.cpu_data(), 0., temp_.mutable_cpu_data());
-  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);
+  caffe_div_to(temp_.count(), top_data, temp_.cpu_data());
   // TODO(cdoersch): The caching is only needed because later in-place layers
   //                 might clobber the data.  Can we skip this if they won't?
   caffe_copy(x_norm_.count(), top_data,
@@ -202,7 +202,7 @@ void BatchNormLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
       spatial_sum_multiplier_.cpu_data(), 0., bottom_diff);
 
   // sum(dE/dY \cdot Y) \cdot Y
-  caffe_mul(temp_.count(), top_data, bottom_diff, bottom_diff);
+  caffe_mul_to(temp_.count(), bottom_diff, top_data);
 
   // sum(dE/dY)-sum(dE/dY \cdot Y) \cdot Y
   caffe_cpu_gemv<Dtype>(CblasNoTrans, channels_ * num, spatial_dim, 1.,
@@ -226,7 +226,7 @@ void BatchNormLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
 
   // note: temp_ still contains sqrt(var(X)+eps), computed during the forward
   // pass.
-  caffe_div(temp_.count(), bottom_diff, temp_.cpu_data(), bottom_diff);
+  caffe_div_to(temp_.count(), bottom_diff, temp_.cpu_data());
 }
 
 
diff --git a/src/caffe/layers/eltwise_layer.cpp b/src/caffe/layers/eltwise_layer.cpp
index 2125616..970215e 100644
--- a/src/caffe/layers/eltwise_layer.cpp
+++ b/src/caffe/layers/eltwise_layer.cpp
@@ -53,7 +53,7 @@ void EltwiseLayer<Dtype>::Forward_cpu(
   case EltwiseParameter_EltwiseOp_PROD:
     caffe_mul(count, bottom[0]->cpu_data(), bottom[1]->cpu_data(), top_data);
     for (int i = 2; i < bottom.size(); ++i) {
-      caffe_mul(count, top_data, bottom[i]->cpu_data(), top_data);
+      caffe_mul_to(count, top_data, bottom[i]->cpu_data());
     }
     break;
   case EltwiseParameter_EltwiseOp_SUM:
@@ -117,14 +117,13 @@ void EltwiseLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
               caffe_copy(count, bottom[j]->cpu_data(), bottom_diff);
               initialized = true;
             } else {
-              caffe_mul(count, bottom[j]->cpu_data(), bottom_diff,
-                        bottom_diff);
+              caffe_mul_to(count, bottom_diff, bottom[j]->cpu_data());
             }
           }
         } else {
           caffe_div(count, top_data, bottom_data, bottom_diff);
         }
-        caffe_mul(count, bottom_diff, top_diff, bottom_diff);
+        caffe_mul_to(count, bottom_diff, top_diff);
         break;
       case EltwiseParameter_EltwiseOp_SUM:
         if (coeffs_[i] == Dtype(1)) {
diff --git a/src/caffe/layers/log_layer.cpp b/src/caffe/layers/log_layer.cpp
index c70a795..7e093ef 100644
--- a/src/caffe/layers/log_layer.cpp
+++ b/src/caffe/layers/log_layer.cpp
@@ -72,7 +72,7 @@ void LogLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
   if (backward_num_scale_ != Dtype(1)) {
     caffe_scal(count, backward_num_scale_, bottom_diff);
   }
-  caffe_mul(count, top_diff, bottom_diff, bottom_diff);
+  caffe_mul_to(count, bottom_diff, top_diff);
 }
 
 #ifdef CPU_ONLY
diff --git a/src/caffe/layers/lrn_layer.cpp b/src/caffe/layers/lrn_layer.cpp
index 210525e..74d42d2 100644
--- a/src/caffe/layers/lrn_layer.cpp
+++ b/src/caffe/layers/lrn_layer.cpp
@@ -148,7 +148,7 @@ void LRNLayer<Dtype>::CrossChannelForward_cpu(
 
   // In the end, compute output
   caffe_powx<Dtype>(scale_.count(), scale_data, -beta_, top_data);
-  caffe_mul<Dtype>(scale_.count(), top_data, bottom_data, top_data);
+  caffe_mul_to<Dtype>(scale_.count(), top_data, bottom_data);
 }
 
 template <typename Dtype>
@@ -195,7 +195,7 @@ void LRNLayer<Dtype>::CrossChannelBackward_cpu(
   Dtype cache_ratio_value = 2. * alpha_ * beta_ / size_;
 
   caffe_powx<Dtype>(scale_.count(), scale_data, -beta_, bottom_diff);
-  caffe_mul<Dtype>(scale_.count(), top_diff, bottom_diff, bottom_diff);
+  caffe_mul_to<Dtype>(scale_.count(), bottom_diff, top_diff);
 
   // go through individual data
   int inverse_pre_pad = size_ - (size_ + 1) / 2;
diff --git a/src/caffe/layers/mvn_layer.cpp b/src/caffe/layers/mvn_layer.cpp
index 8fe4ef8..68d2638 100644
--- a/src/caffe/layers/mvn_layer.cpp
+++ b/src/caffe/layers/mvn_layer.cpp
@@ -66,7 +66,7 @@ void MVNLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
           variance_.cpu_data(), sum_multiplier_.cpu_data(), 0.,
           temp_.mutable_cpu_data());
 
-    caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);
+    caffe_div_to(temp_.count(), top_data, temp_.cpu_data());
   }
 }
 
@@ -94,7 +94,7 @@ void MVNLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
     caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num, dim, 1, 1.,
           mean_.cpu_data(), sum_multiplier_.cpu_data(), 0.,
           bottom_diff);
-    caffe_mul(temp_.count(), top_data, bottom_diff, bottom_diff);
+    caffe_mul_to(temp_.count(), bottom_diff, top_data);
 
     caffe_cpu_gemv<Dtype>(CblasNoTrans, num, dim, 1., top_diff,
             sum_multiplier_.cpu_data(), 0., mean_.mutable_cpu_data());
@@ -112,7 +112,7 @@ void MVNLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
         variance_.cpu_data(), sum_multiplier_.cpu_data(), 0.,
         temp_.mutable_cpu_data());
 
-    caffe_div(temp_.count(), bottom_diff, temp_.cpu_data(), bottom_diff);
+    caffe_div_to(temp_.count(), bottom_diff, temp_.cpu_data());
   } else {
     caffe_cpu_gemv<Dtype>(CblasNoTrans, num, dim, 1. / dim, top_diff,
       sum_multiplier_.cpu_data(), 0., mean_.mutable_cpu_data());
diff --git a/src/caffe/layers/power_layer.cpp b/src/caffe/layers/power_layer.cpp
index d99b77c..559950f 100644
--- a/src/caffe/layers/power_layer.cpp
+++ b/src/caffe/layers/power_layer.cpp
@@ -87,7 +87,7 @@ void PowerLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
       }
     }
     if (diff_scale_ != Dtype(0)) {
-      caffe_mul(count, top_diff, bottom_diff, bottom_diff);
+      caffe_mul_to(count, bottom_diff, top_diff);
     }
   }
 }
diff --git a/src/caffe/layers/softmax_layer.cpp b/src/caffe/layers/softmax_layer.cpp
index f60e9b0..9bc9695 100644
--- a/src/caffe/layers/softmax_layer.cpp
+++ b/src/caffe/layers/softmax_layer.cpp
@@ -53,7 +53,7 @@ void SoftmaxLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
         top_data, sum_multiplier_.cpu_data(), 0., scale_data);
     // division
     for (int j = 0; j < channels; j++) {
-      caffe_div(inner_num_, top_data, scale_data, top_data);
+      caffe_div_to(inner_num_, top_data, scale_data);
       top_data += inner_num_;
     }
   }
@@ -82,7 +82,7 @@ void SoftmaxLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
         -1., sum_multiplier_.cpu_data(), scale_data, 1., bottom_diff + i * dim);
   }
   // elementwise multiplication
-  caffe_mul(top[0]->count(), bottom_diff, top_data, bottom_diff);
+  caffe_mul_to(top[0]->count(), bottom_diff, top_data);
 }
 
 
diff --git a/src/caffe/util/im2col.cpp b/src/caffe/util/im2col.cpp
index 114a86c..07ca804 100644
--- a/src/caffe/util/im2col.cpp
+++ b/src/caffe/util/im2col.cpp
@@ -55,12 +55,12 @@ void im2col_cpu(const Dtype* data_im, const int channels,
 }
 
 // Explicit instantiation
-template void im2col_cpu<float>(const float* data_im, const int channels,
+template __attribute__((target_clones("arch=core-avx2","default"))) void im2col_cpu<float>(const float* data_im, const int channels,
     const int height, const int width, const int kernel_h, const int kernel_w,
     const int pad_h, const int pad_w, const int stride_h,
     const int stride_w, const int dilation_h, const int dilation_w,
     float* data_col);
-template void im2col_cpu<double>(const double* data_im, const int channels,
+template __attribute__((target_clones("arch=core-avx2","default"))) void im2col_cpu<double>(const double* data_im, const int channels,
     const int height, const int width, const int kernel_h, const int kernel_w,
     const int pad_h, const int pad_w, const int stride_h,
     const int stride_w, const int dilation_h, const int dilation_w,
diff --git a/src/caffe/util/math_functions.cpp b/src/caffe/util/math_functions.cpp
index 71c0227..4f92d94 100644
--- a/src/caffe/util/math_functions.cpp
+++ b/src/caffe/util/math_functions.cpp
@@ -138,6 +138,17 @@ void caffe_add<double>(const int n, const double* a, const double* b,
   vdAdd(n, a, b, y);
 }
 
+
+template <>
+void caffe_add_to<float>(const int n, float* a, const float* b) {
+  vsAdd_to(n, a, b);
+}
+
+template <>
+void caffe_add_to<double>(const int n, double* a, const double* b) {
+  vdAdd_to(n, a, b);
+}
+
 template <>
 void caffe_sub<float>(const int n, const float* a, const float* b,
     float* y) {
@@ -162,6 +173,17 @@ void caffe_mul<double>(const int n, const double* a, const double* b,
   vdMul(n, a, b, y);
 }
 
+
+template <>
+void caffe_mul_to<float>(const int n, float* a, const float* b) {
+  vsMul_to(n, a, b);
+}
+
+template <>
+void caffe_mul_to<double>(const int n, double* a, const double* b) {
+  vdMul_to(n, a, b);
+}
+
 template <>
 void caffe_div<float>(const int n, const float* a, const float* b,
     float* y) {
@@ -175,6 +197,16 @@ void caffe_div<double>(const int n, const double* a, const double* b,
 }
 
 template <>
+void caffe_div_to<float>(const int n, float* a, const float* b) {
+  vsDiv_to(n, a, b);
+}
+
+template <>
+void caffe_div_to<double>(const int n, double* a, const double* b) {
+  vdDiv_to(n, a, b);
+}
+
+template <>
 void caffe_powx<float>(const int n, const float* a, const float b,
     float* y) {
   vsPowx(n, a, b, y);
